{
  "experiments": [
    {
      "feature_dimensions": "one_to_one",
      "number_of_trials": 20,
      "kfold_splits": 7,
      "datasets": [
        "data/some-dataset.csv",
        "data/some-dataset.txt"
      ],
      "target_feature": "some-feature-name",
      "models": [
        {
          "name": "MarkovModel",
          "epochs": 1,
          "optimization_config": {
            "alpha": {"min": 1e-5, "max": 1.0}
          }
        },
        {
          "name": "VariableOrderMarkovModel",
          "epochs": 1,
          "optimization_config": {
            "alpha": {"min": 1e-5, "max": 1.0},
            "max_context_length": {"min": 2, "max": 5}
          }
        },
        {
          "name": "LSTMModel",
          "epochs": 200,
          "optimization_config": {
            "lr": {"min": 1e-3, "max": 1e-2},
            "num_layers": {"min": 1, "max": 2},
            "hidden_dim": {"values": [512, 1024]},
            "embedding_dim": {"values": [128, 256, 512]},
            "batch_size": {"values": [32, 64]}
          }
        },
        {
          "name": "LSTMModelWithAttention",
          "epochs": 100,
          "optimization_config": {
            "lr": {"min": 1e-3, "max": 1e-2},
            "num_layers": {"min": 1, "max": 1},
            "hidden_dim": {"values": [128, 256, 512]},
            "embedding_dim": {"values": [128, 512, 1024]},
            "batch_size": {"values": [32, 64]}
          }
        },
        {
          "name": "TransformerModel",
          "epochs": 100,
          "optimization_config": {
            "lr": {"min": 1e-3, "max": 1e-2},
            "nhead": {"values": [8, 16, 32]},
            "num_layers": {"min": 1, "max": 1},
            "dim_feedforward": {"values": [256, 512, 1024]},
            "embed_size": {"values": [128, 256]},
            "batch_size": {"values": [32, 64]}
          }
        },
        {
          "name": "GPTModel",
          "epochs": 100,
          "optimization_config": {
            "lr": {"min": 1e-3, "max": 1e-2},
            "nhead": {"values": [2, 4]},
            "num_layers": {"min": 1, "max": 2},
            "dim_feedforward": {"values": [128, 256, 512]},
            "batch_size": {"values": [32, 64]}
          }
        }
      ]
    },
    {
      "feature_dimensions": "many_to_one",
      "number_of_trials": 20,
      "kfold_splits": 7,
      "source_features": [
        "some-feature-name",
        "some-feature-name-2"
      ],
      "target_feature": "some-feature-name",
      "datasets": [
        "data/some-dataset.csv"
      ],
      "models": [
        {
          "name": "MultiLSTMModel",
          "epochs": 100,
          "optimization_config": {
            "lr": {"min": 1e-4, "max": 1e-1},
            "num_layers": {"min": 1, "max": 1},
            "embedding_dim": {"values": [256, 512, 1024]},
            "hidden_dim": {"values": [256, 512, 1024]},
            "batch_size": {"values": [32, 64]}
          }
        },
        {
          "name": "MultiLSTMAttentionModel",
          "epochs": 100,
          "optimization_config": {
            "lr": {"min": 1e-3, "max": 1e-2},
            "num_layers": {"min": 1, "max": 2},
            "embedding_dim": {"values": [256, 512]},
            "hidden_dim": {"values": [128, 256, 512]},
            "batch_size": {"values": [32, 64]}
          }
        },
        {
          "name": "MultiTransformerModel",
          "epochs": 200,
          "optimization_config": {
            "lr": {"min": 1e-3, "max": 1e-2},
            "nhead": {"values": [16, 32, 64]},
            "embedding_dim": {"values": [64, 256, 1024]},
            "num_layers": {"min": 1, "max": 1},
            "dim_feedforward": {"values": [256, 512, 1024]},
            "batch_size": {"values": [32, 64]}
          }
        },
        {
          "name": "MultiGPTModel",
          "epochs": 200,
          "optimization_config": {
            "lr": {"min": 1e-3, "max": 1e-2},
            "nhead": {"values": [16, 32, 64]},
            "embedding_dim": {"values": [64, 128, 256]},
            "num_layers": {"min": 1, "max": 1},
            "dim_feedforward": {"values": [256, 512, 1024]},
            "batch_size": {"values": [32, 64]}
          }
        }
      ]
    }
  ]
}
